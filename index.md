![imagen](images/cropped-CVPR-banner_2023.png)

**June 19, 2023, Monday. 2nd day of CVPR**, Vancouver, Canada.
Starts at **10 am [Eastern Time](https://time.is/ET)**;  4 pm [Europe Time](https://time.is/CET).  
Held in conjunction with the [IEEE Conference on Computer Vision and Pattern Recognition](http://cvpr2023.thecvf.com/) 2023, as part of the track: CV for non-traditional modalities.

<b>Welcome to the 4th International Workshop on Event-Based Vision!</b>  

## Important Dates

- Paper submission deadline: ~~March 20, 2023 (23:59h PST).  [Submission website (CMT)](https://cmt3.research.microsoft.com/EVENTVISION2023)~~
- Demo abstract submission: ~~March 20, 2023 (23:59h PST)~~
- Notification to authors: ~~April 3, 2023~~
- Camera-ready paper: ~~April 14, 2023 (deadline by IEEE)~~
- <span style="color:red">~~[Early-bird registration **April 30th** (23:59h ET)](https://cvpr2023.thecvf.com/Conferences/2023/Pricing2)~~</span>
- Standard registration begins May 1st.
- <b>Workshop day: **June 19, 2023. 2nd day of CVPR**. Full day workshop.</b>


## Objectives

<div style="text-align: justify">
This workshop is dedicated to event-based cameras, smart cameras, and algorithms processing data from these sensors. Event-based cameras are bio-inspired sensors with the key advantages of microsecond temporal resolution, low latency, very high dynamic range, and low power consumption. Because of these advantages, event-based cameras open frontiers that are unthinkable with standard frame-based cameras (which have been the main sensing technology for the past 60 years). These revolutionary sensors enable the design of a new class of algorithms to track a baseball in the moonlight, build a flying robot with the agility of a bee, and perform structure from motion in challenging lighting conditions and at remarkable speeds. These sensors became commercially available in 2008 and are slowly being adopted in computer vision and robotics. In recent years they have received attention from large companies, e.g., the event-sensor company Prophesee collaborated with Intel and Bosch on a high spatial resolution sensor, Samsung announced mass production of a sensor to be used on hand-held devices, and they have been used in various applications on neuromorphic chips such as IBM’s TrueNorth and Intel’s Loihi. The workshop also considers novel vision sensors, such as pixel processor arrays (PPAs), which perform massively parallel processing near the image plane. Because early vision computations are carried out on-sensor, the resulting systems have high speed and low-power consumption, enabling new embedded vision applications in areas such as robotics, AR/VR, automotive, gaming, surveillance, etc. This workshop will cover the sensing hardware, as well as the processing and learning methods needed to take advantage of the above-mentioned novel cameras.
</div>

## Topics Covered

- Event-based / neuromorphic vision.
- Algorithms: motion estimation, visual odometry, SLAM, 3D reconstruction, image intensity reconstruction, optical flow estimation, recognition, feature/object detection, visual tracking, calibration, sensor fusion (video synthesis, visual-inertial odometry, etc.).
- Model-based, embedded, or learning-based approaches.
- Event-based signal processing, representation, control, bandwidth control.
- Event-based active vision, event-based sensorimotor integration.
- Event camera datasets and/or simulators.
- Applications in: robotics (navigation, manipulation, drones...), automotive, IoT, AR/VR, space science, inspection, surveillance, crowd counting, physics, biology.
- Biologically-inspired vision and smart cameras.
- Near-focal plane processing, such as pixel processor arrays (PPAs).
- Novel hardware (cameras, neuromorphic processors, etc.) and/or software platforms, such as fully event-based systems (end-to-end).
- New trends and challenges in event-based and/or biologically-inspired vision (SNNs, etc.).
- Event-based vision for computational photography.
- A longer list of related topics is available in the table of content of the [List of Event-based Vision Resources](https://github.com/uzh-rpg/event-based_vision_resources) 

## Schedule

The tentative schedule is the following:

Time (local) | Session
--- | ---
8:00 | Session 1: Event-based cameras and neuromorphic computing (Invited speakers)
10:10 | Coffee break
10:30 | Session 2: Poster session: contributed papers and courtesy presentations (as posters). Live Demonstrations.
12:30 | Lunch break
13:30 | Session 3: Applications, Algorithms and Architectures (Invited speakers)
15:30 | Coffee break
16:00 | Session 4: Industrial Session (Invited speakers).
17:45 | Award Ceremony and Final Panel Discussion.
18:00 | End

<!--
- Session 1: Event-based cameras and neuromorphic computing (Invited speakers)
  - Coffee break 
- Session 2: Poster session: contributed papers and courtesy presentations (as posters). Live Demonstrations.
  - Lunch break
- Session 3: Applications, Algorithms and Architectures (Invited speakers)
  - Coffee break 
- Session 4: Industrial Session (Invited speakers).
- Award Ceremony and Final Panel Discussion.
-->

<a name="accepted-papers"></a>   
## Accepted Papers
1. [M3ED: Multi-Robot, Multi-Sensor, Multi-Environment Event Dataset](papers/2023CVPRW_M3ED_Multi-Robot_Multi-Sensor_Multi-Environment_Event_Dataset.pdf), <b><a href="https://m3ed.io/"><span style="color:tomato;">Dataset</span></a></b>,  <b><a href="https://github.com/daniilidis-group/m3ed"><span style="color:tomato;">Code</span></a></b>
2. [PDAVIS: Bio-inspired Polarization Event Camera](papers/2023CVPRW_PDAVIS_Bio-inspired_Polarization_Event_Camera.pdf),  and [Suppl mat](papers/2023CVPRW_PDAVIS_Bio-inspired_Polarization_Event_Camera_supp.pdf)
3. [Asynchronous Events-based Panoptic Segmentation using Graph Mixer Neural Network](papers/2023CVPRW_Asynchronous_Events-based_Panoptic_Segmentation_using_Graph_Mixer_Neural_Network.pdf), and [Suppl mat](papers/2023CVPRW_Asynchronous_Events-based_Panoptic_Segmentation_using_Graph_Mixer_Neural_Network_supp.pdf),  <b><a href="https://github.com/sanket0707/GNN-Mixer.git"><span style="color:tomato;">Code and data</span></a></b>
4. [Shining light on the DVS pixel: A tutorial and discussion about biasing and optimization](papers/2023CVPRW_Shining_light_on_the_DVS_pixel_A_tutorial.pdf), <b><a href="https://docs.google.com/spreadsheets/d/1XaS3hkcjlbSG5gaMnlAy89rbsomILDgu/edit#gid=1310047800"><span style="color:tomato;">Tool</span></a></b> 
5. [Event-IMU fusion strategies for faster-than-IMU estimation throughput](papers/2023CVPRW_Event-IMU_fusion_strategies_for_faster-than-IMU_estimation_throughput.pdf),  and [Suppl mat](papers/2023CVPRW_Event-IMU_fusion_strategies_for_faster-than-IMU_estimation_throughput_supp.zip) 
6. [Fast Trajectory End-Point Prediction with Event Cameras for Reactive Robot Control](papers/2023CVPRW_Fast_Trajectory_End-Point_Prediction_with_Event_Cameras_for_Reactive_Robot_Control.pdf),  and [Suppl mat](papers/2023CVPRW_Fast_Trajectory_End-Point_Prediction_with_Event_Cameras_for_Reactive_Robot_Control_supp.zip), <b><a href="https://github.com/event-driven-robotics/end-point-prediction"><span style="color:tomato;">Code</span></a></b>
7. [Exploring Joint Embedding Architectures and Data Augmentations for Self-Supervised Representation Learning in Event-Based Vision](papers/2023CVPRW_Exploring_Joint_Embedding_Architectures_and_Data_Augmentations.pdf),  and [Suppl mat](papers/2023CVPRW_Exploring_Joint_Embedding_Architectures_and_Data_Augmentations_supp.zip), <b><a href="https://github.com/Barchid/exploring_event_ssl"><span style="color:tomato;">Code</span></a></b>
8. [Event-based Blur Kernel Estimation For Blind Motion Deblurring](papers/2023CVPRW_Event-based_Blur_Kernel_Estimation_For_Blind_Motion_Deblurring.pdf)
9. [Neuromorphic Event-based Facial Expression Recognition](papers/2023CVPRW_Neuromorphic_Event-based_Facial_Expression_Recognition.pdf), <b><a href="https://github.com/miccunifi/NEFER"><span style="color:tomato;">Dataset</span></a></b>
10. [Low-latency monocular depth estimation using event timing on neuromorphic
hardware](papers/2023CVPRW_Low-latency_monocular_depth_estimation_using_event_timing_on_neuromorphic_hardware.pdf)
11. [Frugal event data: how small is too small? A human performance assessment with shrinking data](papers/2023CVPRW_Frugal_event_data_How_small_is_too_small.pdf)
12. [Flow cytometry with event-based vision and spiking neuromorphic hardware](papers/2023CVPRW_Flow_cytometry_with_event-based_vision_and_spiking_neuromorphic_hardware.pdf),  <b><a href="https://github.com/stevenabreu7/dvs_flow"><span style="color:tomato;">Code</span></a></b>
13. [How Many Events Make an Object? Improving Single-frame Object Detection on the 1 Mpx Dataset](papers/2023CVPRW_How_Many_Events_Make_an_Object.pdf),  and [Suppl mat](papers/2023CVPRW_How_Many_Events_Make_an_Object_supp.pdf)
14. [EVREAL: Towards a Comprehensive Benchmark and Analysis Suite for Event-based Video Reconstruction](papers/2023CVPRW_EVREAL_Towards_a_Comprehensive_Benchmark_and_Analysis_Suite_for_Event-based_Video_Reconstruction.pdf),  and [Suppl mat](papers/2023CVPRW_EVREAL_Towards_a_Comprehensive_Benchmark_and_Analysis_Suite_for_Event-based_Video_Reconstruction_supp.zip), <b><a href="https://ercanburak.github.io/evreal.html"><span style="color:tomato;">Code</span></a></b>
15. [X-maps: Direct Depth Lookup for Event-based Structured Light Systems](papers/2023CVPRW_X-Maps_Direct_Depth_Lookup_for_Event-based_Structured_Light_Systems.pdf), <b><a href="https://github.com/fraunhoferhhi/X-maps"><span style="color:tomato;">Code</span></a></b>
16. [PEDRo: an Event-based Dataset for Person Detection in Robotics](papers/2023CVPRW_PEDRo_An_Event-based_Dataset_for_Person_Detection_in_Robotics.pdf), <b><a href="https://github.com/SSIGPRO/PEDRo-Event-Based-Dataset"><span style="color:tomato;">Dataset</span></a></b>
17. [Density Invariant Contrast Maximization for Neuromorphic Earth Observations](papers/2023CVPRW_Density_Invariant_Contrast_Maximization_for_Neuromorphic_Earth_Observations.pdf), and [Suppl mat](papers/2023CVPRW_Density_Invariant_Contrast_Maximization_for_Neuromorphic_Earth_Observations_supp.pdf),  <b><a href="https://github.com/neuromorphicsystems/event_warping"><span style="color:tomato;">Code</span></a></b>
18. [Entropy Coding-based Lossless Compression of Asynchronous Event Sequences](papers/2023CVPRW_Entropy_Coding-based_Lossless_Compression_of_Asynchronous_Event_Sequences.pdf),  and [Suppl mat](papers/2023CVPRW_Entropy_Coding-based_Lossless_Compression_of_Asynchronous_Event_Sequences_supp.pdf)
19. [MoveEnet: Online High-Frequency Human Pose Estimation with an Event Camera](papers/2023CVPRW_MoveEnet_Online_High-Frequency_Human_Pose_Estimation_with_an_Event_Camera.pdf),  and [Suppl mat](papers/2023CVPRW_MoveEnet_Online_High-Frequency_Human_Pose_Estimation_with_an_Event_Camera_supp.zip), <b><a href="https://github.com/event-driven-robotics/hpe-core"><span style="color:tomato;">Code</span></a></b>
20. [Predictive Coding Light: learning compact visual codes by combining excitatory and inhibitory spike timing-dependent plasticity](papers/2023CVPRW_Predictive_Coding_Light.pdf)
21. [Neuromorphic Optical Flow and Real-time Implementation with Event Cameras](papers/2023CVPRW_Neuromorphic_Optical_Flow_and_Real-time_Implementation_with_Event_Cameras.pdf),  and [Suppl mat](papers/2023CVPRW_Neuromorphic_Optical_Flow_and_Real-time_Implementation_with_Event_Cameras_supp.pdf)
22. [HUGNet: Hemi-Spherical Update Graph Neural Network applied to low-latency event-based optical flow](papers/2023CVPRW_HUGNet_Hemi-Spherical_Update_Graph_Neural_Network_applied_to_low-latency_event-based_optical_flow.pdf),  and [Suppl mat](papers/2023CVPRW_HUGNet_Hemi-Spherical_Update_Graph_Neural_Network_applied_to_low-latency_event-based_optical_flow_supp.pdf)
23. [End-to-end Neuromorphic Lip-reading](papers/2023CVPRW_End-to-end_Neuromorphic_Lip-reading.pdf)
24. [Sparse-E2VID: A Sparse Convolutional Model for Event-Based Video Reconstruction Trained with Real Event Noise](papers/2023CVPRW_Sparse-E2VID_A_Sparse_Convolutional_Model_for_Event-Based_Video_Reconstruction.pdf),  and [Suppl mat](papers/2023CVPRW_Sparse-E2VID_A_Sparse_Convolutional_Model_for_Event-Based_Video_Reconstruction_supp.zip), <b><a href="https://youtu.be/sFH9zp6kuWE"><span style="color:tomato;">Video</span></a></b>
25. [Within-Camera Multilayer Perceptron DVS Denoising](papers/2023CVPRW_Within-Camera_Multilayer_Perceptron_DVS_Denoising.pdf),  and [Suppl mat](papers/2023CVPRW_Within-Camera_Multilayer_Perceptron_DVS_Denoising_supp.pdf)
26. [Interpolation-Based Event Visual Data Filtering Algorithms](papers/2023CVPRW_Interpolation-Based_Event_Visual_Data_Filtering_Algorithms.pdf),  <b><a href="https://github.com/vision-agh/DVS_FilterInterpolation"><span style="color:tomato;">Code</span></a></b>


## Live Demonstrations
1. [PINK: Polarity-based Anti-flicker for Event Cameras](papers/2023CVPRW_Live_Demonstration_PINK_Polarity-based_Anti-flicker_for_Event_Cameras.pdf), <b><a href="https://youtu.be/5UdU0PLZaf8"><span style="color:tomato;">Video</span></a></b>
2. [Event-based Visual Microphone](papers/2023CVPRW_Live_Demonstration_Event-based_Visual_Microphone.pdf),  and [Suppl mat](papers/2023CVPRW_Live_Demonstration_Event-based_Visual_Microphone_supp.zip)
3. [E2P--Events to Polarization Reconstruction from PDAVIS Events](papers/2023CVPRW_Live_Demonstration_E2P_Events_to_Polarization_Reconstruction_from_PDAVIS_Events.pdf),  and [Suppl mat](papers/2023CVPRW_Live_Demonstration_E2P_Events_to_Polarization_Reconstruction_from_PDAVIS_Events_supp.zip), <b><a href="https://github.com/SensorsINI/e2p"><span style="color:tomato;">Code</span></a></b>
4. [SCAMP-7](papers/2023CVPRW_Live_Demonstration_Scamp-7.pdf)
5. [ANN vs SNN vs Hybrid Architectures for Event-based Real-time Gesture Recognition and Optical Flow Estimation](papers/2023CVPRW_Live_Demonstration_ANN_vs_SNN_vs_Hybrid_Architectures_for_Event-based_Real-time.pdf)
6. [Tangentially Elongated Gaussian Belief Propagation for Event-based Incremental Optical Flow Estimation](papers/2023CVPRW_Live_Demonstration_TEGBP_for_Event-based_Incremental_Optical_Flow_Estimation.pdf), <b><a href="https://github.com/DensoITLab/tegbp"><span style="color:tomato;">Code</span></a></b>
7. [Real-time Event-based Speed Detection using Spiking Neural Networks](papers/2023CVPRW_Live_Demonstration_Real-time_Event-based_Speed_Detection_using_Spiking_Neural_Networks.pdf)
8. [Integrating Event-based Hand Tracking Into TouchFree Interactions](papers/2023CVPRW_Live_Demonstration_Integrating_Event-based_Hand_Tracking_Into_TouchFree_Interactions.pdf),  and [Suppl mat](papers/2023CVPRW_Live_Demonstration_Integrating_Event-based_Hand_Tracking_Into_TouchFree_Interactions_supp.zip)


<!--
## Call for Papers

<p><div style="text-align: justify">
  Research papers and demos are solicited in, but not limited to, the topics listed above.
</div></p>

  - <b>Paper</b> submissions must adhere to the CVPR 2023 paper submission style, format and length restrictions. 
  See the <a href="https://cvpr2023.thecvf.com/Conferences/2023/AuthorGuidelines">author guidelines</a> and <a href="https://media.icml.cc/Conferences/CVPR2023/cvpr2023-author_kit-v1_1-1.zip">template</a> provided by the CVPR 2023 main conference. See also the policy of <a href="https://iccv2023.thecvf.com/policies-361500-2-20-15.php">Dual/Double Submissions of concurrently-reviewed conferences, such as ICCV</a>.  
  - For <b>demo</b> abstract submission, authors are encouraged to submit an abstract of up to 2 pages using the same template as CVPR 2023 paper submissions.

<p><div style="text-align: justify">
  A double blind peer-review process of the submissions received is carried out via CMT.
  Accepted papers will be published open access through the Computer Vision Foundation (CVF) (see <a href="https://openaccess.thecvf.com/CVPR2019_workshops/CVPR2019_EventVision">examples from CVPR Workshop 2019</a> <a href="https://openaccess.thecvf.com/CVPR2021_workshops/EventVision">and 2021</a>).
  For the accepted papers we encourage authors to write a paragraph about ethical considerations and impact of their work.
</div></p>
-->

### Courtesy presentations (in the poster session) 
<div style="text-align: justify">
  We also invite courtesy presentations of papers relevant to the workshop that are accepted at CVPR main conference or at other peer-reviewed conferences or journals. 
  These presentations provide visibility to your work and help building a community around the topics of the workshop. These contributions will be checked for relevance to the workshop, but will not undergo a complete review, and will not be published in the workshop proceedings. 
  Please contact the organizers to make arrangements to showcase your work at the workshop.  
</div>

<!--
## Reviewer Acknowledgement
[We thank our reviewers](slides/CVPRW21_Reviewers_ack.pdf) for a thorough review process.
-->

## Organizers

![organizers](images/workshop_organizers_60.jpg)

- [Guillermo Gallego](http://www.guillermogallego.es), TU Berlin, Einstein Center Digital Future, [Science of Intelligence Excellence Cluster (SCIoI)](https://www.scienceofintelligence.de/), Germany.
- [Davide Scaramuzza](http://rpg.ifi.uzh.ch/people_scaramuzza.html), University of Zurich, Switzerland.
- [Kostas Daniilidis](https://www.cis.upenn.edu/~kostas), University of Pennsylvania, USA. 
- [Cornelia Fermüller](http://users.umiacs.umd.edu/~fer), University of Maryland, USA.
- [Davide Migliore](https://www.linkedin.com/in/davidemigliore), [Prophesee](https://www.prophesee.ai/), France.


## FAQs
<ul>
  <li><b>What is an event camera?</b> Watch this <a href="https://youtu.be/LauQ6LWTkxM">video explanation</a>.</li>
  <li><b>What are possible applications of event cameras?</b> Check the <b><a href="https://arxiv.org/abs/1904.08405">TPAMI 2022 review paper</a></b>.
  </li>
  <li><b>Where can I buy an event camera?</b> From <a href="https://github.com/uzh-rpg/event-based_vision_resources#companies_sftwr"> Inivation, Prophesee, CelePixel, Insightness</a>.</li>
  <li><b>Are there datasets and simulators that I can play with?</b> Yes, <a href="http://rpg.ifi.uzh.ch/davis_data.html">Dataset</a>. <a href="http://rpg.ifi.uzh.ch/esim.html">Simulator</a>. <a href="https://github.com/uzh-rpg/event-based_vision_resources#datasets">More</a>.</li>
  <li><b>Is there any online course about event-based vision?</b> Yes, check this <a href="https://sites.google.com/view/guillermogallego/teaching/event-based-robot-vision"> course at TU Berlin</a>.</li>
  <li><b>What is the SCAMP sensor?</b> Read this <a href="https://personalpages.manchester.ac.uk/staff/p.dudek/scamp/">page explanation</a>.</li>
  <li><b>What are possible applications of the scamp sensor?</b> Some applications can be found <a href="https://personalpages.manchester.ac.uk/staff/p.dudek/scamp/default.htm#Applications">here</a>.</li>
  <li><b>Where can I buy a SCAMP sensor?</b> It is not commercially available. Contact Prof. <a href="https://personalpages.manchester.ac.uk/staff/p.dudek/pdudek.htm">Piotr Dudek</a>.</li>
  <li><b>Where can I find more information?</b> Check out this <a href="https://github.com/uzh-rpg/event-based_vision_resources">List of Event-based Vision Resources</a>.</li>
</ul>

## Past Related Workshops
<ul>
  <li><a href="https://sites.google.com/view/eventsensorfusion2022/home">MFI 2022 First Neuromorphic Event Sensor Fusion Workshop</a>.
    <a href="https://youtube.com/playlist?list=PLVtZ8f-q0U5gXhjN4inwWZi66bp5vp-lN">Videos</a></li>
  <li><a href="https://www.tinyml.org/event/tinyml-neuromorphic-engineering-forum/">tinyML Neuromorphic Engineering Forum</a>.
    <a href="https://www.youtube.com/playlist?list=PLeisuBi-nfBM5HayCqF4KMBaJciV5UkLX">Videos</a></li>  
  <li><a href="https://sites.google.com/view/telluride-2022/home">2022 Telluride Neuromorphic workshop.</a></li>
  <li><a href="https://sites.google.com/view/tellurideneuromorphic2021/home">2021 Telluride Neuromorphic workshop.</a></li>
  <li><a href="https://tub-rip.github.io/eventvision2021/slides/ICCV2021Tutorial.pdf">ICCV 2021 Tutorial. Introduction to Event Detection Cameras</a>.</li>
  <li><b><a href="https://tub-rip.github.io/eventvision2021/">CVPR 2021 Third International Workshop on Event-based Vision</a>.
    <a href="https://www.youtube.com/playlist?list=PLeXWz-g2If95mjNpA-y-WIoDaoB8WtmE7">Videos</a></b></li>
  <li><a href="https://sites.google.com/view/onsvp-icra-2021-workshop/home">ICRA 2021 Workshop On- and Near-sensor Vision Processing, from Photons to Applications (ONSVP)</a>.</li>
  <li><a href="https://robotics.sydney.edu.au/icra-workshop/">ICRA 2020 Workshop on Sensing, Estimating and Understanding the Dynamic World. Session on Event-based camera companies iniVation and Prophesee</a>.</li>
  <li><a href="https://sites.google.com/view/unconventional-sensors">ICRA 2020 Workshop on Unconventional Sensors in Robotics</a>.
      <a href="https://www.youtube.com/playlist?list=PLtW5yHT6tQuD4sLzkldzZEyQ4hz77K64-">Videos</a></li>
  <li><b><a href="http://rpg.ifi.uzh.ch/CVPR19_event_vision_workshop.html">CVPR 2019 Second International Workshop on Event-based Vision and Smart Cameras</a>.
      <a href="https://www.youtube.com/playlist?list=PLeXWz-g2If97iGiuBHmnW8IFIxwvSeCHx">Videos</a></b></li>
  <li><a href="https://www.jmartel.net/irosws-home">IROS 2018 Workshop on Unconventional Sensing and Processing for Robotic Visual Perception</a>.</li>
  <li><b><a href="http://rpg.ifi.uzh.ch/ICRA17_event_vision_workshop.html">ICRA 2017 First International Workshop on Event-based Vision</a>.
      <a href="https://www.youtube.com/playlist?list=PLeXWz-g2If94k8mw6GcKU5C9PUgM1sK0U">Videos</a></b></li>
  <li><a href="http://innovative-sensing.mit.edu/">ICRA 2015 Workshop on Innovative Sensing for Robotics, with focus on Neuromorphic Sensors</a>.</li>
  <li><a href="http://www.rit.edu/kgcoe/iros15workshop/papers/IROS2015-WASRoP-Invited-04-slides.pdf">Event-Based Vision for High-Speed Robotics (slides)</a> IROS 2015, Workshop on Alternative Sensing for Robot Perception.</li>
  <li><a href="http://telluride.iniforum.ch">The Telluride Neuromorphic Cognition Engineering Workshops</a>.</li>
  <li><a href="http://capocaccia.iniforum.ch">Capo Caccia Workshops toward Cognitive Neuromorphic Engineering</a>.</li>
</ul>

## Supported by 

<a href="https://www.scienceofintelligence.de"><img src="images/ScioI_Logo_L.svg" width="348"></a>
