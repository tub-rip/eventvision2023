![imagen](images/cropped-CVPR-banner_2023.png)

**June 19, 2023, Monday. 2nd day of CVPR**, Vancouver, Canada.
Starts at **8 am Local time**;  4 pm [Europe Time](https://time.is/CET).  
Held in conjunction with the [IEEE Conference on Computer Vision and Pattern Recognition](http://cvpr2023.thecvf.com/) 2023.

<b>Welcome to the 4th International Workshop on Event-Based Vision!</b>  

[![group photo by S. Shiba](/images/CVPRW2023_EventVision_group_picture.jpg)](https://photos.app.goo.gl/z5gMLG2AF6U5qHcY7)
Many thanks to all who contributed and made this workshop possible!

## <span style="color:tomato;">[Photo Album of the Workshop](https://photos.app.goo.gl/z5gMLG2AF6U5qHcY7)</span>

## <span style="color:tomato;">[Videos! YouTube Playlist](https://www.youtube.com/playlist?list=PLeXWz-g2If96iotpzgBNNTr9VA6hG-LLK)</span>

<!--
## <span style="color:tomato;">[Introduction slides](https://docs.google.com/presentation/d/1qPRGIAz6ZRtJmVEYhfHjJZZc5I9F45Rvz4zjbyktWVw/edit?usp=sharing)</span>
-->

## Speakers
![imagen](/images/workshop_speakers_2023.jpg)

<!--
## Location: Hybrid
- On site (Vancouver Convention Center): Room <b>West 209</b>
- [Virtual: see CVPR virtual platform for zoom link](https://cvpr2023.thecvf.com/virtual/2023/workshop/18456)
-->

## Schedule (starts at 8 am Local time)

<!--
The tentative schedule is the following:

Time (local) | Session
--- | ---
8:00 | [Welcome slides](https://docs.google.com/presentation/d/1qPRGIAz6ZRtJmVEYhfHjJZZc5I9F45Rvz4zjbyktWVw/edit?usp=sharing), [Video](https://youtu.be/V0OXHBv5TsM)
8:00 | Session 1: Event-based cameras and neuromorphic computing (Invited speakers)
10:10 | Coffee break. Set up posters.
10:30 | Session 2: Poster session: contributed papers, demos and courtesy presentations (as posters).
12:30 | Lunch break
13:30 | Session 3: Applications, Algorithms and Architectures (Invited speakers)
15:30 | Coffee break
16:00 | Session 4: Industrial Session (Invited speakers).
17:45 | [Award ceremony](https://docs.google.com/presentation/d/1DF2-hyGVP2G1wgXJC-n8yF2lZMf3633YOyjT-h_dKVs/edit?usp=sharing), [Video](https://youtu.be/vEcZbdyC6gM)
18:00 | End
-->

<!--
- Session 1: Event-based cameras and neuromorphic computing (Invited speakers)
  - Coffee break
- Session 2: Poster session: contributed papers and courtesy presentations (as posters). Live Demonstrations.
  - Lunch break
- Session 3: Applications, Algorithms and Architectures (Invited speakers)
  - Coffee break
- Session 4: Industrial Session (Invited speakers).
- Award Ceremony and Final Panel Discussion.
-->

<!--
### Session #1 (8:00 h, Vancouver time)
* [Kaushik Roy (Purdue University)](https://scholar.google.com/citations?hl=en&user=to4P8KgAAAAJ&view_op=list_works&sortby=pubdate): Re-thinking Computing with Neuro-Inspired Learning: Algorithms, Sensors, and Hardware Architecture. <b><a href="https://youtu.be/ONXg4irpIbc"><span style="color:tomato;">Video</span></a></b>, <b><a href="slides/2023CVPRW_Kaushik_Roy.pdf"><span style="color:tomato;">Slides</span></a></b>
* [Ryad Benosman (Meta)](https://scholar.google.ch/citations?user=_ZTFUooAAAAJ&hl=en&oi=sra): The Interplay Between Events and Frames: A Comprehensive Explanation. <b><a href="https://youtu.be/skPW4igKWOo"><span style="color:tomato;">Video</span></a></b>, <b><a href="slides/2023CVPRW_Ryad_Benosman.pdf"><span style="color:tomato;">Slides</span></a></b>
* [Katie Schuman (University of Tennessee)](https://catherineschuman.com/): A Workflow for Low-Power Neuromorphic Computing for Event-Based Vision Applications. <b><a href="https://youtu.be/Qyvj98ugkDs"><span style="color:tomato;">Video</span></a></b>
* [Andre van Schaik (Western Sydney University)](https://scholar.google.com/citations?user=Ud0G048AAAAJ&hl=en): Applications of Event-based Vision at the International Centre for Neuromorphic Systems. <b><a href="https://youtu.be/z0Fd_B8TrNk"><span style="color:tomato;">Video</span></a></b>

### Session #2 (10:30 h, Vancouver time)
* <b>Poster session</b>. Accepted papers, demos and courtesy presentations. See links below.
* <b>Posters #1 to #49 in the West exhibit hall</b> are booked for this workshop from 10:10 to 13:30 h.
All posters must go up at 10:10 h and be taken down at 13:30 h.

### Session #3 (13:30 h, Vancouver time)
* [Felix Heide (Princeton University)](https://www.cs.princeton.edu/~fheide/): Neural Nanophotonic Cameras. <b><a href="https://youtu.be/gBBc4cVc44Q"><span style="color:tomato;">Video</span></a></b>
* [Arren Glover (Italian Institute of Technology)](https://www.iit.it/people/arren-glover): Real-time, Speed-invariant, Vision for Robotics. <b><a href="https://youtu.be/Ff6Xj_zriqc"><span style="color:tomato;">Video</span></a></b>, <b><a href="slides/2023CVPRW_Glover_novideo.pdf"><span style="color:tomato;">Slides</span></a></b>
* [NeuroPAC](https://www.neuropac.info/). <b><a href="https://youtu.be/3mzNeLgB_mI"><span style="color:tomato;">Video</span></a></b>
* [Cornelia Fermüller (University of Maryland)](https://scholar.google.ch/citations?user=0gEOJSEAAAAJ&hl=en): When do neuromorphic sensors outperform cameras? Learning from dynamic features. <b><a href="https://youtu.be/sSnV7TuMMk4"><span style="color:tomato;">Video</span></a></b>, <b><a href="https://docs.google.com/presentation/d/1kr3-YO34iIAnjgKbdHLsXJlIt0AI8gc0/edit?usp=drive_link&ouid=118409722950191552935&rtpof=true&sd=true"><span style="color:tomato;">Slides</span></a></b>
* [Daniel Gehrig (Scaramuzza's Lab, University of Zurich)](https://danielgehrig18.github.io/): Efficient event processing with geometric deep learning. <b><a href="https://youtu.be/V6uGnuznIg4"><span style="color:tomato;">Video</span></a></b>, <b><a href="slides/2023CVPRW_Daniel_Gehrig.pdf"><span style="color:tomato;">Slides</span></a></b>
* [Guillermo Gallego (TU Berlin, ECDF, SCIoI)](https://sites.google.com/view/guillermogallego/research/event-based-vision): Event-based Robot Vision for Autonomous Systems and Animal Observation. <b><a href="https://youtu.be/31bfaACIotA"><span style="color:tomato;">Video</span></a></b>, <b><a href="slides/2023CVPRW_Guillermo_Gallego.pdf"><span style="color:tomato;">Slides</span></a></b>
* [Kenneth Chaney and Fernando Cladera (Daniilidis' Lab, University of Pennsylvania)](https://m3ed.io/index.html): M3ED: Multi-robot, Multi-Sensor, Multi-Environment Event Dataset. <b><a href="https://youtu.be/0krz1q6WCyA"><span style="color:tomato;">Video</span></a></b>
* [Boxin Shi (Peking University)](https://ci.idm.pku.edu.cn/): NeuCAP: Neuromorphic Camera Aided Photography. <b><a href="https://youtu.be/sHHMJdDLCKc"><span style="color:tomato;">Video</span></a></b>, <b><a href="slides/2023CVPRW_Shi_NeuCamAidedPhoto.pdf"><span style="color:tomato;">Slides</span></a></b>

### Session #4 (16:00 h, Vancouver time)
* [Yulia Sandamirskaya and Andreas Wild (Intel Labs)](https://www.intel.com/content/www/us/en/research/neuromorphic-computing.html): Visual Processing with Loihi 2. <b><a href="https://youtu.be/tbq_c5THUuM"><span style="color:tomato;">Video</span></a></b>, <b><a href="slides/2023CVPRW_IntelNeuromorphicComputingLab.pdf"><span style="color:tomato;">Slides</span></a></b>
* [Kynan Eng (iniVation)](https://inivation.com/): Beyond Frames and Events: Next Generation Visual Sensing. <b><a href="https://youtu.be/tv-GqKg4Mak"><span style="color:tomato;">Video</span></a></b>
* [Atsumi Niwa (SONY)](https://www.linkedin.com/in/atsumi-niwa-a1227692/): Event-based Vision Sensor and On-chip Processing Development. <b><a href="https://youtu.be/_2GNcxGcqbU"><span style="color:tomato;">Video</span></a></b>, <b><a href="https://docs.google.com/presentation/d/1XrubklM7Z7Rl1zpOWnNUKOFuSFVi9QJO/edit?usp=sharing&ouid=107010905684016788580&rtpof=true&sd=true"><span style="color:tomato;">Slides</span></a></b>
* [Andreas Suess (OmniVision Technologies)](https://www.linkedin.com/in/andreas-suess-03a510162/): Towards hybrid event/image vision. <b><a href="https://youtu.be/IAfaieVqvzY"><span style="color:tomato;">Video</span></a></b>
* [Nandan Nayampally (Brainchip)](https://brainchip.com/): Enabling Ultra-Low Power Edge Inference and On-Device Learning with Akida. <b><a href="https://youtu.be/aZr_F-Ne75k"><span style="color:tomato;">Video</span></a></b>
* [Christoph Posch (Prophesee)](https://www.prophesee.ai/): Event sensors for embedded AI vision applications. <b><a href="https://youtu.be/wRWCwJBF534"><span style="color:tomato;">Video</span></a></b>
-->

<table style="width: 100%">
  <colgroup>
    <col span="1" style="width: 10%;">
    <col span="1" style="width: 90%;">
  </colgroup>
  <thead>
    <tr>
      <td><b>Time</b></td>
      <td><b>Speaker and Title</b></td>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td bgcolor="WhiteSmoke">08:00</td>
      <td bgcolor="WhiteSmoke"><b>SESSION 1</b></td>
    </tr>
    <tr>
      <td style="background-color:rgb(233, 252, 233);">08:00</td>
      <td style="background-color:rgb(233, 252, 233);">
        <b> Welcome and Organization</b>.
        <b><a href="https://youtu.be/V0OXHBv5TsM"><span style="color:tomato;">Video</span></a></b>,
        <b><a href="https://docs.google.com/presentation/d/1qPRGIAz6ZRtJmVEYhfHjJZZc5I9F45Rvz4zjbyktWVw/edit?usp=sharing"> Slides</a></b>
      </td>
    </tr>
    <tr>
      <td>08:05</td>
      <td><a href="https://scholar.google.com/citations?hl=en&user=to4P8KgAAAAJ&view_op=list_works&sortby=pubdate">Kaushik Roy (Purdue University)</a>.
        Re-thinking Computing with Neuro-Inspired Learning: Algorithms, Sensors, and Hardware Architecture. 
        <b><a href="https://youtu.be/ONXg4irpIbc"><span style="color:tomato;">Video</span></a></b>,
        <b><a href="slides/2023CVPRW_Kaushik_Roy.pdf"><span style="color:tomato;">Slides</span></a></b>
      </td>
    </tr>
    <tr>
      <td>08:35</td>
      <td><a href="https://scholar.google.ch/citations?user=_ZTFUooAAAAJ&hl=en">Ryad Benosman (Meta)</a>.
        The Interplay Between Events and Frames: A Comprehensive Explanation. 
        <b><a href="https://youtu.be/skPW4igKWOo"><span style="color:tomato;">Video</span></a></b>, 
        <b><a href="slides/2023CVPRW_Ryad_Benosman.pdf"><span style="color:tomato;">Slides</span></a></b>
      </td>
    </tr>
    <tr>
      <td>09:05</td>
      <td><a href="https://catherineschuman.com/">Katie Schuman (University of Tennessee)</a>.
        A Workflow for Low-Power Neuromorphic Computing for Event-Based Vision Applications. 
        <b><a href="https://youtu.be/Qyvj98ugkDs"><span style="color:tomato;">Video</span></a></b>
      </td>
    </tr>
    <tr>
      <td>09:35</td>
      <td><a href="https://scholar.google.com/citations?user=Ud0G048AAAAJ&hl=en">Andre van Schaik (Western Sydney University)</a>.
        Applications of Event-based Vision at the International Centre for Neuromorphic Systems. 
        <b><a href="https://youtu.be/z0Fd_B8TrNk"><span style="color:tomato;">Video</span></a></b>
      </td>
    </tr>
    <tr>
      <td bgcolor="LavenderBlush">10:00</td>
      <td bgcolor="LavenderBlush">Coffee break. Set up posters</td>
    </tr>
    <tr>
      <td bgcolor="WhiteSmoke">10:30</td>
      <td bgcolor="WhiteSmoke"><b>SESSION 2</b></td>
    </tr>
    <tr>
      <td bgcolor="LemonChiffon">10:30</td>
      <td bgcolor="LemonChiffon"><b>Poster session</b>: contributed papers, demos and courtesy papers (as posters). See links below. Boards 1 -- 49.
      </td>
    </tr>
    <tr>
      <td bgcolor="LavenderBlush">12:30</td>
      <td bgcolor="LavenderBlush">Lunch break</td>
    </tr>
    <tr>
      <td bgcolor="WhiteSmoke">13:30</td>
      <td bgcolor="WhiteSmoke"><b>SESSION 3</b></td>
    </tr>
    <tr>
      <td>13:30</td>
      <td><a href="https://www.cs.princeton.edu/~fheide/">Felix Heide (Princeton University)</a>.
        Neural Nanophotonic Cameras.
        <b><a href="https://youtu.be/gBBc4cVc44Q"><span style="color:tomato;">Video</span></a></b>
      </td>
    </tr>
    <tr>
      <td>13:55</td>
      <td><a href="https://www.iit.it/people/arren-glover">Arren Glover (Italian Institute of Technology)</a>.
        Real-time, Speed-invariant, Vision for Robotics. 
        <b><a href="https://youtu.be/Ff6Xj_zriqc"><span style="color:tomato;">Video</span></a></b>, 
        <b><a href="slides/2023CVPRW_Glover_novideo.pdf"><span style="color:tomato;">Slides</span></a></b>
      </td>
    </tr>
    <tr>
      <td>14:20</td>
      <td><a href="https://www.neuropac.info/">NeuroPAC</a>.
        <b><a href="https://youtu.be/3mzNeLgB_mI"><span style="color:tomato;">Video</span></a></b> 
      </td>
    </tr>
    <tr>
      <td>14:25</td>
      <td><a href="https://scholar.google.ch/citations?user=0gEOJSEAAAAJ&hl=en">Cornelia Fermüller (University of Maryland)</a>.
        When do neuromorphic sensors outperform cameras? Learning from dynamic features.
        <b><a href="https://youtu.be/sSnV7TuMMk4"><span style="color:tomato;">Video</span></a></b>, 
        <b><a href="https://docs.google.com/presentation/d/1kr3-YO34iIAnjgKbdHLsXJlIt0AI8gc0/edit?usp=drive_link&ouid=118409722950191552935&rtpof=true&sd=true"><span style="color:tomato;">Slides</span></a></b>
      </td>
    </tr>
    <tr>
      <td>14:30</td>
      <td><a href="https://danielgehrig18.github.io/">Daniel Gehrig (Scaramuzza's Lab, University of Zurich)</a>.
        Efficient event processing with geometric deep learning. 
        <b><a href="https://youtu.be/V6uGnuznIg4"><span style="color:tomato;">Video</span></a></b>, 
        <b><a href="slides/2023CVPRW_Daniel_Gehrig.pdf"><span style="color:tomato;">Slides</span></a></b>
      </td>
    </tr>
    <tr>
      <td>14:35</td>
      <td><a href="https://sites.google.com/view/guillermogallego/research/event-based-vision">Guillermo Gallego (TU Berlin, ECDF, SCIoI)</a>.
        Event-based Robot Vision for Autonomous Systems and Animal Observation. 
        <b><a href="https://youtu.be/31bfaACIotA"><span style="color:tomato;">Video</span></a></b>,
        <b><a href="slides/2023CVPRW_Guillermo_Gallego.pdf"><span style="color:tomato;">Slides</span></a></b>
      </td>
    </tr>
    <tr>
      <td>14:40</td>
      <td><a href="https://m3ed.io/index.html">Kenneth Chaney and Fernando Cladera (Daniilidis' Lab, University of Pennsylvania)</a>.
        M3ED: Multi-robot, Multi-Sensor, Multi-Environment Event Dataset. 
        <b><a href="https://youtu.be/0krz1q6WCyA"><span style="color:tomato;">Video</span></a></b>
      </td>
    </tr>
    <tr>
      <td>14:45</td>
      <td><a href="https://ci.idm.pku.edu.cn/">Boxin Shi (Peking University)</a>.
        NeuCAP: Neuromorphic Camera Aided Photography. 
        <b><a href="https://youtu.be/sHHMJdDLCKc"><span style="color:tomato;">Video</span></a></b>, 
        <b><a href="slides/2023CVPRW_Shi_NeuCamAidedPhoto.pdf"><span style="color:tomato;">Slides</span></a></b>
      </td>
    </tr>
    <tr>
      <td bgcolor="LavenderBlush">15:10</td>
      <td bgcolor="LavenderBlush">Coffee break</td>
    </tr>
    <tr>
      <td bgcolor="WhiteSmoke">16:00</td>
      <td bgcolor="WhiteSmoke"><b>SESSION 4</b></td>
    </tr>
    <tr>
      <td>16:00</td>
      <td><a href="https://www.intel.com/content/www/us/en/research/neuromorphic-computing.html">Yulia Sandamirskaya and Andreas Wild (Intel Labs)</a>.
        Visual Processing with Loihi 2.
        <b><a href="https://youtu.be/tbq_c5THUuM"><span style="color:tomato;">Video</span></a></b>,
        <b><a href="slides/2023CVPRW_IntelNeuromorphicComputingLab.pdf"><span style="color:tomato;">Slides</span></a></b>
      </td>
    </tr>
    <tr>
      <td>16:20</td>
      <td><a href="https://inivation.com/">Kynan Eng (iniVation)</a>.
        Beyond Frames and Events: Next Generation Visual Sensing.
        <b><a href="https://youtu.be/tv-GqKg4Mak"><span style="color:tomato;">Video</span></a></b>
      </td>
    </tr>
    <tr>
      <td>16:40</td>
      <td><a href="https://www.linkedin.com/in/atsumi-niwa-a1227692/">Atsumi Niwa (SONY)</a>.
        Event-based Vision Sensor and On-chip Processing Development. 
        <b><a href="https://youtu.be/_2GNcxGcqbU"><span style="color:tomato;">Video</span></a></b>, 
        <b><a href="https://docs.google.com/presentation/d/1XrubklM7Z7Rl1zpOWnNUKOFuSFVi9QJO/edit?usp=sharing&ouid=107010905684016788580&rtpof=true&sd=true"><span style="color:tomato;">Slides</span></a></b>
      </td>
    </tr>
    <tr>
      <td>17:00</td>
      <td><a href="https://www.linkedin.com/in/andreas-suess-03a510162/">Andreas Suess (OmniVision Technologies)</a>.
        Towards hybrid event/image vision. 
        <b><a href="https://youtu.be/IAfaieVqvzY"><span style="color:tomato;">Video</span></a></b>
      </td>
    </tr>
    <tr>
      <td>17:20</td>
      <td><a href="https://brainchip.com/">Nandan Nayampally (Brainchip)</a>.
        Enabling Ultra-Low Power Edge Inference and On-Device Learning with Akida. 
        <b><a href="https://youtu.be/aZr_F-Ne75k"><span style="color:tomato;">Video</span></a></b>
      </td>
    </tr>
    <tr>
      <td>17:35</td>
      <td><a href="https://www.prophesee.ai/">Christoph Posch (Prophesee)</a>.
        Event sensors for embedded AI vision applications. 
        <b><a href="https://youtu.be/wRWCwJBF534"><span style="color:tomato;">Video</span></a></b>
      </td>
    </tr>
    <tr>
      <td style="background-color:rgb(233, 252, 233);">17:50</td>
      <td style="background-color:rgb(233, 252, 233);">
        <b> Welcome and Organization</b>.
        <b><a href="https://youtu.be/vEcZbdyC6gM"><span style="color:tomato;">Video</span></a></b>, 
        <b><a href="https://docs.google.com/presentation/d/1DF2-hyGVP2G1wgXJC-n8yF2lZMf3633YOyjT-h_dKVs/edit?usp=sharing"> Slides</a></b>
      </td>
    </tr>
  </tbody>
</table>

<a name="accepted-papers"></a>
## [Proceedings at The Computer Vision Foundation (CVF)](https://openaccess.thecvf.com/CVPR2023_workshops/EventVision)

### List of accepted papers

1. [M3ED: Multi-Robot, Multi-Sensor, Multi-Environment Event Dataset](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Chaney_M3ED_Multi-Robot_Multi-Sensor_Multi-Environment_Event_Dataset_CVPRW_2023_paper.pdf), <b><a href="https://m3ed.io/"><span style="color:tomato;">Dataset</span></a></b>,  <b><a href="https://github.com/daniilidis-group/m3ed"><span style="color:tomato;">Code</span></a></b>
2. [Asynchronous Events-based Panoptic Segmentation using Graph Mixer Neural Network](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Kachole_Asynchronous_Events-Based_Panoptic_Segmentation_Using_Graph_Mixer_Neural_Network_CVPRW_2023_paper.pdf), and [Suppl mat](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/supplemental/Kachole_Asynchronous_Events-Based_Panoptic_CVPRW_2023_supplemental.pdf),  <b><a href="https://github.com/sanket0707/GNN-Mixer.git"><span style="color:tomato;">Code and data</span></a></b>
3. [Event-IMU fusion strategies for faster-than-IMU estimation throughput](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Chamorro_Event-IMU_Fusion_Strategies_for_Faster-Than-IMU_Estimation_Throughput_CVPRW_2023_paper.pdf), and [Suppl mat](papers/2023CVPRW_Event-IMU_fusion_strategies_for_faster-than-IMU_estimation_throughput_supp.zip)
4. [Fast Trajectory End-Point Prediction with Event Cameras for Reactive Robot Control](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Monforte_Fast_Trajectory_End-Point_Prediction_With_Event_Cameras_for_Reactive_Robot_CVPRW_2023_paper.pdf), and [Suppl mat](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/supplemental/Monforte_Fast_Trajectory_End-Point_CVPRW_2023_supplemental.zip), <b><a href="https://github.com/event-driven-robotics/end-point-prediction"><span style="color:tomato;">Code</span></a></b>
5. [Exploring Joint Embedding Architectures and Data Augmentations for Self-Supervised Representation Learning in Event-Based Vision](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Barchid_Exploring_Joint_Embedding_Architectures_and_Data_Augmentations_for_Self-Supervised_Representation_CVPRW_2023_paper.pdf),  and [Suppl mat](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/supplemental/Barchid_Exploring_Joint_Embedding_CVPRW_2023_supplemental.zip), [Poster](papers/2023CVPRW_Exploring_Joint_Embedding_Architectures_and_Data_Augmentations_poster.pdf) <b><a href="https://github.com/Barchid/exploring_event_ssl"><span style="color:tomato;">Code</span></a></b>
6. [Event-based Blur Kernel Estimation For Blind Motion Deblurring](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Nakabayashi_Event-Based_Blur_Kernel_Estimation_for_Blind_Motion_Deblurring_CVPRW_2023_paper.pdf)
7. [Neuromorphic Event-based Facial Expression Recognition](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Berlincioni_Neuromorphic_Event-Based_Facial_Expression_Recognition_CVPRW_2023_paper.pdf), <b><a href="https://github.com/miccunifi/NEFER"><span style="color:tomato;">Dataset</span></a></b>
8. [Low-latency monocular depth estimation using event timing on neuromorphic hardware](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Chiavazza_Low-Latency_Monocular_Depth_Estimation_Using_Event_Timing_on_Neuromorphic_Hardware_CVPRW_2023_paper.pdf)
9. [Frugal event data: how small is too small? A human performance assessment with shrinking data](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Gruel_Frugal_Event_Data_How_Small_Is_Too_Small_A_Human_CVPRW_2023_paper.pdf), <b><a href="papers/2023CVPRW_Frugal_event_data_How_small_is_too_small_poster.pdf"><span style="color:tomato;">Poster</span></a></b>
10. [Flow cytometry with event-based vision and spiking neuromorphic hardware](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Abreu_Flow_Cytometry_With_Event-Based_Vision_and_Spiking_Neuromorphic_Hardware_CVPRW_2023_paper.pdf),  <b><a href="https://github.com/stevenabreu7/dvs_flow"><span style="color:tomato;">Code</span></a></b>,  <b><a href="papers/2023CVPRW_Flow_cytometry_with_event-based_vision_and_spiking_neuromorphic_hardware_poster.pdf"><span style="color:tomato;">Poster</span></a></b>
11. [How Many Events Make an Object? Improving Single-frame Object Detection on the 1 Mpx Dataset](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Kugele_How_Many_Events_Make_an_Object_Improving_Single-Frame_Object_Detection_CVPRW_2023_paper.pdf),  and [Suppl mat](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/supplemental/Kugele_How_Many_Events_CVPRW_2023_supplemental.pdf), <b><a href="https://github.com/boschresearch/ecod"><span style="color:tomato;">Code</span></a></b>
12. [EVREAL: Towards a Comprehensive Benchmark and Analysis Suite for Event-based Video Reconstruction](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Ercan_EVREAL_Towards_a_Comprehensive_Benchmark_and_Analysis_Suite_for_Event-Based_CVPRW_2023_paper.pdf),  and [Suppl mat](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/supplemental/Ercan_EVREAL_Towards_a_CVPRW_2023_supplemental.zip), <b><a href="https://ercanburak.github.io/evreal.html"><span style="color:tomato;">Code</span></a></b>, <b><a href="papers/2023CVPRW_EVREAL_Towards_a_Comprehensive_Benchmark_and_Analysis_Suite_for_Event-based_Video_Reconstruction_poster.pdf"><span style="color:tomato;">Poster</span></a></b>
13. [X-maps: Direct Depth Lookup for Event-based Structured Light Systems](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Morgenstern_X-Maps_Direct_Depth_Lookup_for_Event-Based_Structured_Light_Systems_CVPRW_2023_paper.pdf), and <a href="papers/2023CVPRW_X-Maps_Direct_Depth_Lookup_for_Event-based_Structured_Light_Systems_poster.pdf"><span style="color:tomato;">Poster</span></a>, <b><a href="https://github.com/fraunhoferhhi/X-maps"><span style="color:tomato;">Code</span></a></b>
14. [PEDRo: an Event-based Dataset for Person Detection in Robotics](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Boretti_PEDRo_An_Event-Based_Dataset_for_Person_Detection_in_Robotics_CVPRW_2023_paper.pdf), <b><a href="https://github.com/SSIGPRO/PEDRo-Event-Based-Dataset"><span style="color:tomato;">Dataset</span></a></b>
15. [Density Invariant Contrast Maximization for Neuromorphic Earth Observations](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Arja_Density_Invariant_Contrast_Maximization_for_Neuromorphic_Earth_Observations_CVPRW_2023_paper.pdf), and [Suppl mat](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/supplemental/Arja_Density_Invariant_Contrast_CVPRW_2023_supplemental.pdf),  <b><a href="https://github.com/neuromorphicsystems/event_warping"><span style="color:tomato;">Code</span></a></b>, <b><a href="papers/2023CVPRW_DICMaxNEO_poster.pdf"><span style="color:tomato;">Poster</span></a></b>
16. [Entropy Coding-based Lossless Compression of Asynchronous Event Sequences](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Schiopu_Entropy_Coding-Based_Lossless_Compression_of_Asynchronous_Event_Sequences_CVPRW_2023_paper.pdf),  and [Suppl mat](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/supplemental/Schiopu_Entropy_Coding-Based_Lossless_CVPRW_2023_supplemental.pdf), [Poster](papers/2023CVPRW_Entropy_Coding-based_Lossless_Compression_of_Asynchronous_Event_Sequences_poster.pdf)
17. [MoveEnet: Online High-Frequency Human Pose Estimation with an Event Camera](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Goyal_MoveEnet_Online_High-Frequency_Human_Pose_Estimation_With_an_Event_Camera_CVPRW_2023_paper.pdf),  and [Suppl mat](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/supplemental/Goyal_MoveEnet_Online_High-Frequency_CVPRW_2023_supplemental.zip), <b><a href="https://github.com/event-driven-robotics/hpe-core"><span style="color:tomato;">Code</span></a></b>
18. [Predictive Coding Light: learning compact visual codes by combining excitatory and inhibitory spike timing-dependent plasticity](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Ndri_Predictive_Coding_Light_Learning_Compact_Visual_Codes_by_Combining_Excitatory_CVPRW_2023_paper.pdf)
19. [Neuromorphic Optical Flow and Real-time Implementation with Event Cameras](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Schnider_Neuromorphic_Optical_Flow_and_Real-Time_Implementation_With_Event_Cameras_CVPRW_2023_paper.pdf),  and [Suppl mat](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/supplemental/Schnider_Neuromorphic_Optical_Flow_CVPRW_2023_supplemental.pdf), [Poster](papers/2023CVPRW_Neuromorphic_Optical_Flow_and_Real-time_Implementation_with_Event_Cameras_poster.pdf), <b><a href="https://youtu.be/jDGDxKabj0o"><span style="color:tomato;">Video</span></a></b>
20. [HUGNet: Hemi-Spherical Update Graph Neural Network applied to low-latency event-based optical flow](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Dalgaty_HUGNet_Hemi-Spherical_Update_Graph_Neural_Network_Applied_to_Low-Latency_Event-Based_CVPRW_2023_paper.pdf),  and [Suppl mat](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/supplemental/Dalgaty_HUGNet_Hemi-Spherical_Update_CVPRW_2023_supplemental.pdf)
21. [End-to-end Neuromorphic Lip-reading](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Bulzomi_End-to-End_Neuromorphic_Lip-Reading_CVPRW_2023_paper.pdf), <b><a href="papers/2023CVPRW_End-to-end_Neuromorphic_Lip-reading_poster.pdf"><span style="color:tomato;">Poster</span></a></b>
22. [Sparse-E2VID: A Sparse Convolutional Model for Event-Based Video Reconstruction Trained with Real Event Noise](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Cadena_Sparse-E2VID_A_Sparse_Convolutional_Model_for_Event-Based_Video_Reconstruction_Trained_CVPRW_2023_paper.pdf),  and [Suppl mat](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/supplemental/Cadena_Sparse-E2VID_A_Sparse_CVPRW_2023_supplemental.zip), <b><a href="https://youtu.be/sFH9zp6kuWE"><span style="color:tomato;">Video</span></a></b>
23. [Interpolation-Based Event Visual Data Filtering Algorithms](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Kowalczyk_Interpolation-Based_Event_Visual_Data_Filtering_Algorithms_CVPRW_2023_paper.pdf),  <b><a href="https://github.com/vision-agh/DVS_FilterInterpolation"><span style="color:tomato;">Code</span></a></b>
24. [Within-Camera Multilayer Perceptron DVS Denoising](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Rios-Navarro_Within-Camera_Multilayer_Perceptron_DVS_Denoising_CVPRW_2023_paper.pdf), [Suppl mat](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/supplemental/Rios-Navarro_Within-Camera_Multilayer_Perceptron_CVPRW_2023_supplemental.pdf), <b><a href="https://github.com/SensorsINI/dnd_hls"><span style="color:tomato;">Code</span></a></b>, <b><a href="papers/2023CVPRW_Within-Camera_Multilayer_Perceptron_DVS_Denoising_poster.pdf"><span style="color:tomato;">Poster</span></a></b>
25. [Shining light on the DVS pixel: A tutorial and discussion about biasing and optimization](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Graca_Shining_Light_on_the_DVS_Pixel_A_Tutorial_and_Discussion_CVPRW_2023_paper.pdf), <b><a href="https://docs.google.com/spreadsheets/d/1XaS3hkcjlbSG5gaMnlAy89rbsomILDgu/edit#gid=1310047800"><span style="color:tomato;">Tool</span></a></b>
26. [PDAVIS: Bio-inspired Polarization Event Camera](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Haessig_PDAVIS_Bio-Inspired_Polarization_Event_Camera_CVPRW_2023_paper.pdf), [Suppl mat](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/supplemental/Haessig_PDAVIS_Bio-Inspired_Polarization_CVPRW_2023_supplemental.pdf), <b><a href="https://github.com/SensorsINI/e2p"><span style="color:tomato;">Code</span></a></b>

### List of accepted live demonstrations

27. [E2P--Events to Polarization Reconstruction from PDAVIS Events](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Delbruck_Live_Demo_E2P-Events_to_Polarization_Reconstruction_From_PDAVIS_Events_CVPRW_2023_paper.pdf),  and [Suppl mat](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/supplemental/Delbruck_Live_Demo_E2P-Events_CVPRW_2023_supplemental.zip), <b><a href="https://github.com/SensorsINI/e2p"><span style="color:tomato;">Code</span></a></b>
28. [PINK: Polarity-based Anti-flicker for Event Cameras](papers/2023CVPRW_Live_Demonstration_PINK_Polarity-based_Anti-flicker_for_Event_Cameras.pdf), <b><a href="https://youtu.be/5UdU0PLZaf8"><span style="color:tomato;">Video</span></a></b>
29. [Event-based Visual Microphone](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Niwa_Live_Demonstration_Event-Based_Visual_Microphone_CVPRW_2023_paper.pdf),  and [Suppl mat](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/supplemental/Niwa_Live_Demonstration_Event-Based_CVPRW_2023_supplemental.zip)
30. [SCAMP-7](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Bose_Live_Demonstration_Scamp-7_CVPRW_2023_paper.pdf)
31. [ANN vs SNN vs Hybrid Architectures for Event-based Real-time Gesture Recognition and Optical Flow Estimation](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Kosta_Live_Demonstration_ANN_vs_SNN_vs_Hybrid_Architectures_for_Event-Based_CVPRW_2023_paper.pdf)
32. [Tangentially Elongated Gaussian Belief Propagation for Event-based Incremental Optical Flow Estimation](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Sekikawa_Live_Demonstration_Tangentially_Elongated_Gaussian_Belief_Propagation_for_Event-Based_Incremental_CVPRW_2023_paper.pdf), <b><a href="https://github.com/DensoITLab/tegbp"><span style="color:tomato;">Code</span></a></b>
33. [Real-time Event-based Speed Detection using Spiking Neural Networks](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Roy_Live_Demonstration_Real-Time_Event-Based_Speed_Detection_Using_Spiking_Neural_Networks_CVPRW_2023_paper.pdf)
34. [Integrating Event-based Hand Tracking Into TouchFree Interactions](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Page_Live_Demonstration_Integrating_Event_Based_Hand_Tracking_Into_TouchFree_Interactions_CVPRW_2023_paper.pdf),  and [Suppl mat](https://openaccess.thecvf.com/content/CVPR2023W/EventVision/supplemental/Page_Live_Demonstration_Integrating_CVPRW_2023_supplemental.zip)

<a name="courtesy-presentations"></a>   
## List of courtesy papers (as posters, during session #2)

1. [Event Collapse in Motion Estimation using Contrast Maximization](https://github.com/tub-rip/event_collapse), by [Shintaro Shiba](http://shibashintaro.com/), from Keio University and TU Berlin. <b><a href="papers/2023CVPRW_EventCollapse_and_OpticalFlow_poster.pdf"><span style="color:tomato;">Poster</span></a></b>
2. [Deep Asynchronous Graph Neural Networks for Events and Frames](https://danielgehrig18.github.io/), by [Daniel Gehrig](https://scholar.google.es/citations?user=FWpgbBsAAAAJ&hl=en), from the University of Zurich.
3. [Animal behavior observation with event cameras](https://arxiv.org/abs/2207.07332), by [Friedhelm Hamann](https://www.scienceofintelligence.de/people/friedhelm-hamann/), from TU Berlin and the Science of Intelligence Excellence Cluster (SCIoI). <b><a href="papers/2023CVPRW_Poster_Animal_Observation.pdf"><span style="color:tomato;">Poster</span></a></b>
4. [Recurrent Vision Transformers for Object Detection with Event Cameras](https://github.com/uzh-rpg/RVT), by [Mathias Gehrig](https://scholar.google.es/citations?user=uTMjaVoAAAAJ&hl=en), from the University of Zurich, CVPR 2023.
5. [Learning to Estimate Two Dense Depths from LiDAR and Event Data](https://link.springer.com/chapter/10.1007/978-3-031-31438-4_34), by [Vincent Brebion](https://www.researchgate.net/profile/Vincent_Brebion) (Université de Technologie de Compiègne), Julien Moreau and Franck Davoine, SCIA 2023. [Project Page (suppl. material, poster, code, dataset, videos)](https://vbrebion.github.io/ALED/). [PDF](https://arxiv.org/abs/2302.14444)
6. [ESS: Learning Event-based Semantic Segmentation from Still Images](https://github.com/uzh-rpg/ess), by [Nico Messikomer](https://messikommernico.github.io/), from the University of Zurich, ECCV 2022.
7. [Multi-event-camera Depth Estimation](https://github.com/tub-rip/dvs_mcemvs), by [Suman Ghosh](https://sumanghosh29.wixsite.com/sumanghosh), from TU Berlin. <b><a href="papers/2023CVPRW_MCEMVS_poster.pdf"><span style="color:tomato;">Poster</span></a></b>
8. [Event-based shape from polarization](https://rpg.ifi.uzh.ch/esfp.html), by [Manasi Muglikar](https://manasi94.github.io/), from the University of Zurich, CVPR 2023.
9. [All-in-focus Imaging from Event Focal Stack](https://youtu.be/9HQLqj4cY7o), by Hanyue Lou, [Minggui Teng](https://tengminggui.cn/), Yixin Yang, and [Boxin Shi](https://ci.idm.pku.edu.cn/), CVPR 2023.
10. [Event-aided Direct Sparse Odometry](https://rpg.ifi.uzh.ch/eds.html), by [Javier Hidalgo-Carrió](https://jhidalgocarrio.github.io/), from the University of Zurich. <b><a href="papers/2023CVPRW_EDS_poster.pdf"><span style="color:tomato;">Poster</span></a></b>
11. [High-fidelity Event-Radiance Recovery via Transient Event Frequency](https://github.com/hjynwa/TEF), by [Jin Han](https://hjynwa.github.io/), Yuta Asano, [Boxin Shi](https://ci.idm.pku.edu.cn/), [Yinqiang Zheng](https://scholar.google.com/citations?user=JD-5DKcAAAAJ&hl), and [Imari Sato](https://scholar.google.com/citations?user=gtfbzYwAAAAJ), from the University of Tokyo, NII and Peking University, CVPR 2023. <b><a href="papers/2023CVPRW_High-fidelity_Event-Radiance_Recovery_via_Transient_Event_Frequency_poster.pdf"><span style="color:tomato;">Poster</span></a></b>

## Papers at the main conference (CVPR 2023)
1. [Adaptive Global Decay Process for Event Cameras](https://openaccess.thecvf.com/content/CVPR2023/html/Nunes_Adaptive_Global_Decay_Process_for_Event_Cameras_CVPR_2023_paper.html), <b><a href="https://github.com/neuromorphic-paris/event_batch"><span style="color:tomato;">Code</span></a></b>
2. [All-in-focus Imaging from Event Focal Stack](https://openaccess.thecvf.com/content/CVPR2023/html/Lou_All-in-Focus_Imaging_From_Event_Focal_Stack_CVPR_2023_paper.html), <b><a href="https://youtu.be/9HQLqj4cY7o"><span style="color:tomato;">Video</span></a></b>
3. [Data-driven Feature Tracking for Event Cameras](https://openaccess.thecvf.com/content/CVPR2023/html/Messikommer_Data-Driven_Feature_Tracking_for_Event_Cameras_CVPR_2023_paper.html), <b><a href="https://youtu.be/dtkXvNXcWRY"><span style="color:tomato;">Video</span></a></b>, <b><a href="https://github.com/uzh-rpg/deep_ev_tracker"><span style="color:tomato;">Code</span></a></b>
4. [Deep Polarization Reconstruction with PDAVIS Events](https://openaccess.thecvf.com/content/CVPR2023/html/Mei_Deep_Polarization_Reconstruction_With_PDAVIS_Events_CVPR_2023_paper.html), <b><a href="https://youtu.be/pdwpfDMUXSE"><span style="color:tomato;">Video</span></a></b>, <b><a href="https://github.com/SensorsINI/e2p"><span style="color:tomato;">Code</span></a></b>
5. [Event-based Blurry Frame Interpolation under Blind Exposure](https://openaccess.thecvf.com/content/CVPR2023/html/Weng_Event-Based_Blurry_Frame_Interpolation_Under_Blind_Exposure_CVPR_2023_paper.html), <b><a href="https://github.com/WarranWeng/EBFI-BE"><span style="color:tomato;">Code</span></a></b>
6. [Event-Based Frame Interpolation with Ad-hoc Deblurring](https://openaccess.thecvf.com/content/CVPR2023/html/Sun_Event-Based_Frame_Interpolation_With_Ad-Hoc_Deblurring_CVPR_2023_paper.html), <b><a href=""><span style="color:tomato;">Video</span></a></b>, <b><a href="https://github.com/AHupuJR/REFID"><span style="color:tomato;">Code</span></a></b>
7. [Event-based Shape from Polarization](https://openaccess.thecvf.com/content/CVPR2023/html/Muglikar_Event-Based_Shape_From_Polarization_CVPR_2023_paper.html), <b><a href="https://youtu.be/sF3Ue2Zkpec"><span style="color:tomato;">Video</span></a></b>, <b><a href="https://rpg.ifi.uzh.ch/esfp.html"><span style="color:tomato;">Code</span></a></b>
8. [Event-based Video Frame Interpolation with Cross-Modal Asymmetric Bidirectional Motion Fields](https://openaccess.thecvf.com/content/CVPR2023/html/Kim_Event-Based_Video_Frame_Interpolation_With_Cross-Modal_Asymmetric_Bidirectional_Motion_Fields_CVPR_2023_paper.html), <b><a href=""><span style="color:tomato;">Video</span></a></b>
9. [Event-guided Person Re-Identification via Sparse-Dense Complementary Learning](https://openaccess.thecvf.com/content/CVPR2023/html/Cao_Event-Guided_Person_Re-Identification_via_Sparse-Dense_Complementary_Learning_CVPR_2023_paper.html), <b><a href="https://github.com/Chengzhi-Cao/SDCL"><span style="color:tomato;">Code</span></a></b>
10. [EventNeRF: Neural Radiance Fields from a Single Colour Event Camera](https://openaccess.thecvf.com/content/CVPR2023/html/Rudnev_EventNeRF_Neural_Radiance_Fields_From_a_Single_Colour_Event_Camera_CVPR_2023_paper.html), <b><a href="https://youtu.be/IDyvmRMGMws"><span style="color:tomato;">Video</span></a></b>, <b><a href="https://4dqv.mpi-inf.mpg.de/EventNeRF/"><span style="color:tomato;">Project page</span></a></b>
11. [EvShutter: Transforming Events for Unconstrained Rolling Shutter Correction](https://openaccess.thecvf.com/content/CVPR2023/html/Erbach_EvShutter_Transforming_Events_for_Unconstrained_Rolling_Shutter_Correction_CVPR_2023_paper.html), <b><a href="https://youtu.be/0S3yT8HWcHw"><span style="color:tomato;">Video</span></a></b>
12. [Frame-Event Alignment and Fusion Network for High Frame Rate Tracking](https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Frame-Event_Alignment_and_Fusion_Network_for_High_Frame_Rate_Tracking_CVPR_2023_paper.html), <b><a href="https://github.com/Jee-King/AFNet"><span style="color:tomato;">Code</span></a></b>
13. [Hierarchical Neural Memory Network for Low Latency Event Processing](https://openaccess.thecvf.com/content/CVPR2023/html/Hamaguchi_Hierarchical_Neural_Memory_Network_for_Low_Latency_Event_Processing_CVPR_2023_paper.html), <b><a href="https://youtu.be/FY-lLmVnbCI"><span style="color:tomato;">Video</span></a></b>, <b><a href="https://hamarh.github.io/hmnet/"><span style="color:tomato;">Project page</span></a></b>
14. [High-fidelity Event-Radiance Recovery via Transient Event Frequency](https://openaccess.thecvf.com/content/CVPR2023/html/Han_High-Fidelity_Event-Radiance_Recovery_via_Transient_Event_Frequency_CVPR_2023_paper.html), <b><a href="https://youtu.be/wf138eAoazE"><span style="color:tomato;">Video</span></a></b>, <b><a href="https://github.com/hjynwa/TEF"><span style="color:tomato;">Code</span></a></b>
15. [Learning Adaptive Dense Event Stereo from the Image Domain](https://openaccess.thecvf.com/content/CVPR2023/html/Cho_Learning_Adaptive_Dense_Event_Stereo_From_the_Image_Domain_CVPR_2023_paper.html)
16. [Learning Event Guided High Dynamic Range Video Reconstruction](https://openaccess.thecvf.com/content/CVPR2023/html/Yang_Learning_Event_Guided_High_Dynamic_Range_Video_Reconstruction_CVPR_2023_paper.html), <b><a href="https://youtu.be/3v_3zW_KGlw"><span style="color:tomato;">Video</span></a></b>
17. [Learning Spatial-Temporal Implicit Neural Representations for Event-Guided Video Super-Resolution](https://openaccess.thecvf.com/content/CVPR2023/html/Lu_Learning_Spatial-Temporal_Implicit_Neural_Representations_for_Event-Guided_Video_Super-Resolution_CVPR_2023_paper.html), <b><a href="https://youtu.be/ty531p2Me7Q"><span style="color:tomato;">Video</span></a></b>
18. [Progressive Spatio-temporal Alignment for Efficient Event-based Motion Estimation](https://openaccess.thecvf.com/content/CVPR2023/html/Huang_Progressive_Spatio-Temporal_Alignment_for_Efficient_Event-Based_Motion_Estimation_CVPR_2023_paper.html), <b><a href="https://github.com/huangxueyan/PEME"><span style="color:tomato;">Code</span></a></b>
19. [Recurrent Vision Transformers for Object Detection with Event Cameras](https://openaccess.thecvf.com/content/CVPR2023/html/Gehrig_Recurrent_Vision_Transformers_for_Object_Detection_With_Event_Cameras_CVPR_2023_paper.html), <b><a href="https://youtu.be/xZ-pNwHxHgY"><span style="color:tomato;">Video</span></a></b>, <b><a href="https://github.com/uzh-rpg/RVT"><span style="color:tomato;">Code</span></a></b>
20. ["Seeing" Electric Network Frequency from Events](https://openaccess.thecvf.com/content/CVPR2023/html/Xu_Seeing_Electric_Network_Frequency_From_Events_CVPR_2023_paper.html), <b><a href="https://xlx-creater.github.io/E-ENF/"><span style="color:tomato;">Project page</span></a></b>
21. [Tangentially Elongated Gaussian Belief Propagation for Event-based Incremental Optical Flow Estimation](https://openaccess.thecvf.com/content/CVPR2023/html/Nagata_Tangentially_Elongated_Gaussian_Belief_Propagation_for_Event-Based_Incremental_Optical_Flow_CVPR_2023_paper.html), <b><a href="https://youtu.be/rClkk5MY33A"><span style="color:tomato;">Video</span></a></b>, <b><a href="https://github.com/DensoITLab/tegbp"><span style="color:tomato;">Code</span></a></b>

## Objectives

<div style="text-align: justify">
This workshop is dedicated to event-based cameras, smart cameras, and algorithms processing data from these sensors. Event-based cameras are bio-inspired sensors with the key advantages of microsecond temporal resolution, low latency, very high dynamic range, and low power consumption. Because of these advantages, event-based cameras open frontiers that are unthinkable with standard frame-based cameras (which have been the main sensing technology for the past 60 years). These revolutionary sensors enable the design of a new class of algorithms to track a baseball in the moonlight, build a flying robot with the agility of a bee, and perform structure from motion in challenging lighting conditions and at remarkable speeds. These sensors became commercially available in 2008 and are slowly being adopted in computer vision and robotics. In recent years they have received attention from large companies, e.g., the event-sensor company Prophesee collaborated with Intel and Bosch on a high spatial resolution sensor, Samsung announced mass production of a sensor to be used on hand-held devices, and they have been used in various applications on neuromorphic chips such as IBM’s TrueNorth and Intel’s Loihi. The workshop also considers novel vision sensors, such as pixel processor arrays (PPAs), which perform massively parallel processing near the image plane. Because early vision computations are carried out on-sensor, the resulting systems have high speed and low-power consumption, enabling new embedded vision applications in areas such as robotics, AR/VR, automotive, gaming, surveillance, etc. This workshop will cover the sensing hardware, as well as the processing and learning methods needed to take advantage of the above-mentioned novel cameras.
</div>

## Topics Covered

- Event-based / neuromorphic vision.
- Algorithms: motion estimation, visual odometry, SLAM, 3D reconstruction, image intensity reconstruction, optical flow estimation, recognition, feature/object detection, visual tracking, calibration, sensor fusion (video synthesis, visual-inertial odometry, etc.).
- Model-based, embedded, or learning-based approaches.
- Event-based signal processing, representation, control, bandwidth control.
- Event-based active vision, event-based sensorimotor integration.
- Event camera datasets and/or simulators.
- Applications in: robotics (navigation, manipulation, drones...), automotive, IoT, AR/VR, space science, inspection, surveillance, crowd counting, physics, biology.
- Biologically-inspired vision and smart cameras.
- Near-focal plane processing, such as pixel processor arrays (PPAs).
- Novel hardware (cameras, neuromorphic processors, etc.) and/or software platforms, such as fully event-based systems (end-to-end).
- New trends and challenges in event-based and/or biologically-inspired vision (SNNs, etc.).
- Event-based vision for computational photography.
- A longer list of related topics is available in the table of content of the [List of Event-based Vision Resources](https://github.com/uzh-rpg/event-based_vision_resources)

<!--
## Call for Papers

<p><div style="text-align: justify">
  Research papers and demos are solicited in, but not limited to, the topics listed above.
</div></p>

  - <b>Paper</b> submissions must adhere to the CVPR 2023 paper submission style, format and length restrictions.
  See the <a href="https://cvpr2023.thecvf.com/Conferences/2023/AuthorGuidelines">author guidelines</a> and <a href="https://media.icml.cc/Conferences/CVPR2023/cvpr2023-author_kit-v1_1-1.zip">template</a> provided by the CVPR 2023 main conference. See also the policy of <a href="https://iccv2023.thecvf.com/policies-361500-2-20-15.php">Dual/Double Submissions of concurrently-reviewed conferences, such as ICCV</a>.  
  - For <b>demo</b> abstract submission, authors are encouraged to submit an abstract of up to 2 pages using the same template as CVPR 2023 paper submissions.

<p><div style="text-align: justify">
  A double blind peer-review process of the submissions received is carried out via CMT.
  Accepted papers will be published open access through the Computer Vision Foundation (CVF) (see <a href="https://openaccess.thecvf.com/CVPR2019_workshops/CVPR2019_EventVision">examples from CVPR Workshop 2019</a> <a href="https://openaccess.thecvf.com/CVPR2021_workshops/EventVision">and 2021</a>).
  For the accepted papers we encourage authors to write a paragraph about ethical considerations and impact of their work.
</div></p>

### Courtesy presentations (in the poster session)
<div style="text-align: justify">
  We also invite courtesy presentations of papers relevant to the workshop that are accepted at CVPR main conference or at other peer-reviewed conferences or journals.
  These presentations provide visibility to your work and help building a community around the topics of the workshop. These contributions will be checked for relevance to the workshop, but will not undergo a complete review, and will not be published in the workshop proceedings.
  Please contact the organizers to make arrangements to showcase your work at the workshop.  
</div>
-->

<!--
## Reviewer Acknowledgement
[We thank our reviewers](slides/CVPRW21_Reviewers_ack.pdf) for a thorough review process.
-->

## Organizers

![organizers](images/workshop_organizers_60.jpg)

- [Guillermo Gallego](http://www.guillermogallego.es), TU Berlin, Einstein Center Digital Future, [Science of Intelligence Excellence Cluster (SCIoI)](https://www.scienceofintelligence.de/), Germany.
- [Davide Scaramuzza](http://rpg.ifi.uzh.ch/people_scaramuzza.html), University of Zurich, Switzerland.
- [Kostas Daniilidis](https://www.cis.upenn.edu/~kostas), University of Pennsylvania, USA.
- [Cornelia Fermüller](http://users.umiacs.umd.edu/~fer), University of Maryland, USA.
- [Davide Migliore](https://www.linkedin.com/in/davidemigliore), [Prophesee](https://www.prophesee.ai/), France.

## Important Dates

- Paper submission deadline: ~~March 20, 2023 (23:59h PST).  [Submission website (CMT)](https://cmt3.research.microsoft.com/EVENTVISION2023)~~
- Demo abstract submission: ~~March 20, 2023 (23:59h PST)~~
- Notification to authors: ~~April 3, 2023~~
- Camera-ready paper: ~~April 14, 2023 (deadline by IEEE)~~
- ~~[Early-bird registration **April 30th** (23:59h ET)](https://cvpr2023.thecvf.com/Conferences/2023/Pricing2)~~
- ~~Standard registration begins May 1st.~~
- <b>Workshop day: **June 19, 2023. 2nd day of CVPR**. Full day workshop.</b>

## FAQs
<ul>
  <li><b>What is an event camera?</b> Watch this <a href="https://youtu.be/LauQ6LWTkxM">video explanation</a>.</li>
  <li><b>What are possible applications of event cameras?</b> Check the <b><a href="https://arxiv.org/abs/1904.08405">TPAMI 2022 review paper</a></b>.
  </li>
  <li><b>Where can I buy an event camera?</b> From <a href="https://github.com/uzh-rpg/event-based_vision_resources#companies_sftwr"> Inivation, Prophesee, CelePixel, Insightness</a>.</li>
  <li><b>Are there datasets and simulators that I can play with?</b> Yes, <a href="http://rpg.ifi.uzh.ch/davis_data.html">Dataset</a>. <a href="http://rpg.ifi.uzh.ch/esim.html">Simulator</a>. <a href="https://github.com/uzh-rpg/event-based_vision_resources#datasets">More</a>.</li>
  <li><b>Is there any online course about event-based vision?</b> Yes, check this <a href="https://sites.google.com/view/guillermogallego/teaching/event-based-robot-vision"> course at TU Berlin</a>.</li>
  <li><b>What is the SCAMP sensor?</b> Read this <a href="https://personalpages.manchester.ac.uk/staff/p.dudek/scamp/">page explanation</a>.</li>
  <li><b>What are possible applications of the scamp sensor?</b> Some applications can be found <a href="https://personalpages.manchester.ac.uk/staff/p.dudek/scamp/default.htm#Applications">here</a>.</li>
  <li><b>Where can I buy a SCAMP sensor?</b> It is not commercially available. Contact Prof. <a href="https://personalpages.manchester.ac.uk/staff/p.dudek/pdudek.htm">Piotr Dudek</a>.</li>
  <li><b>Where can I find more information?</b> Check out this <a href="https://github.com/uzh-rpg/event-based_vision_resources">List of Event-based Vision Resources</a>.</li>
</ul>

## Upcoming Related Workshops
<ul>
  <li><a href="https://www.gdr-isis.fr/index.php/reunion/499/">Caméra à événements appliquée à la robotique</a>, Sorbonne University, Paris (Nov. 16th, 2023).
  <a href="slides/2023-11_Workshop_event_cameras_Sorbonne.pdf">Announcement</a></li>
</ul>

## Past Related Workshops
<ul>
  <li><a href="https://imagesensors.org/2023-international-image-sensor-workshop/">2023 Int. Image Sensor Workshop (IISW)</a>.</li>
  <li><a href="https://sites.google.com/view/eventsensorfusion2022/home">MFI 2022 First Neuromorphic Event Sensor Fusion Workshop</a>.
    <a href="https://youtube.com/playlist?list=PLVtZ8f-q0U5gXhjN4inwWZi66bp5vp-lN">Videos</a></li>
  <li><a href="https://www.tinyml.org/event/tinyml-neuromorphic-engineering-forum/">tinyML Neuromorphic Engineering Forum</a>.
    <a href="https://www.youtube.com/playlist?list=PLeisuBi-nfBM5HayCqF4KMBaJciV5UkLX">Videos</a></li>  
  <li><a href="https://sites.google.com/view/telluride-2022/home">2022 Telluride Neuromorphic workshop.</a></li>
  <li><a href="https://sites.google.com/view/tellurideneuromorphic2021/home">2021 Telluride Neuromorphic workshop.</a></li>
  <li><a href="https://tub-rip.github.io/eventvision2021/slides/ICCV2021Tutorial.pdf">ICCV 2021 Tutorial. Introduction to Event Detection Cameras</a>.</li>
  <li><b><a href="https://tub-rip.github.io/eventvision2021/">CVPR 2021 Third International Workshop on Event-based Vision</a>.
    <a href="https://www.youtube.com/playlist?list=PLeXWz-g2If95mjNpA-y-WIoDaoB8WtmE7">Videos</a></b></li>
  <li><a href="https://sites.google.com/view/onsvp-icra-2021-workshop/home">ICRA 2021 Workshop On- and Near-sensor Vision Processing, from Photons to Applications (ONSVP)</a>.</li>
  <li><a href="https://robotics.sydney.edu.au/icra-workshop/">ICRA 2020 Workshop on Sensing, Estimating and Understanding the Dynamic World. Session on Event-based camera companies iniVation and Prophesee</a>.</li>
  <li><a href="https://sites.google.com/view/unconventional-sensors">ICRA 2020 Workshop on Unconventional Sensors in Robotics</a>.
      <a href="https://www.youtube.com/playlist?list=PLtW5yHT6tQuD4sLzkldzZEyQ4hz77K64-">Videos</a></li>
  <li><b><a href="http://rpg.ifi.uzh.ch/CVPR19_event_vision_workshop.html">CVPR 2019 Second International Workshop on Event-based Vision and Smart Cameras</a>.
      <a href="https://www.youtube.com/playlist?list=PLeXWz-g2If97iGiuBHmnW8IFIxwvSeCHx">Videos</a></b></li>
  <li><a href="https://www.jmartel.net/irosws-home">IROS 2018 Workshop on Unconventional Sensing and Processing for Robotic Visual Perception</a>.</li>
  <li><b><a href="http://rpg.ifi.uzh.ch/ICRA17_event_vision_workshop.html">ICRA 2017 First International Workshop on Event-based Vision</a>.
      <a href="https://www.youtube.com/playlist?list=PLeXWz-g2If94k8mw6GcKU5C9PUgM1sK0U">Videos</a></b></li>
  <li><a href="http://innovative-sensing.mit.edu/">ICRA 2015 Workshop on Innovative Sensing for Robotics, with focus on Neuromorphic Sensors</a>.</li>
  <li><a href="http://www.rit.edu/kgcoe/iros15workshop/papers/IROS2015-WASRoP-Invited-04-slides.pdf">Event-Based Vision for High-Speed Robotics (slides)</a> IROS 2015, Workshop on Alternative Sensing for Robot Perception.</li>
  <li><a href="http://telluride.iniforum.ch">The Telluride Neuromorphic Cognition Engineering Workshops</a>.</li>
  <li><a href="http://capocaccia.iniforum.ch">Capo Caccia Workshops toward Cognitive Neuromorphic Engineering</a>.</li>
</ul>

## Supported by

<a href="https://www.scienceofintelligence.de"><img src="images/ScioI_Logo_L.svg" width="348"></a>
